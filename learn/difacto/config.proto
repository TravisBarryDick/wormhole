//  @file   config.proto
//  @brief  configuration for factorization machine

package dmlc.fm;

message Config {

  // --  basic settings - input & output  --

  // Both train_data and val_data can be either a directory or a wildcard
  // filename such as "part-[0-9].*". also support hdfs:// and s3://
  optional string train_data = 1;
  optional string val_data = 2;

  // data format. support libsvm, crb, criteo, adfea, ...
  optional string data_format = 4 [default = "libsvm"];

  // model output
  optional string model_out = 5;

  // save model for every *save_model* data pass. if 0, then only save the last
  // one
  optional int32 save_model = 6 [default = 0];

  // model input
  optional string model_in = 7;

  // load model from iter = *load_model*. if 0, then load the last one
  optional int32 load_model = 8 [default = 0];


  // output of the prediction results. if specified, then do
  // prediction. otherwise run the training job.
  optional string pred_out = 9;


  // --  basic settings - objective and optimization  --

  // - w -

  // elastic-net regularizer:
  //   lambda_l1 * ||w||_1 + .5 * lambda_l2 * ||w||_2^2
  optional float lambda_l1 = 11 [default = 1];
  optional float lambda_l2 = 12 [default = 0];

  // learning rate, often in the format lr_eta / (lr_beta + x), where x depends
  // on the updater, such as sqrt(iter), or the cumulative gradient on adagrad
  optional float lr_eta = 14 [default = .01];

  // - V -

  message Embedding {
    // -- model --

    // the dimension k
    optional int32 dim = 1;

    // features with occur frequency < threshold are ignored in the
    // embeding. namely k = 0
    optional int32 threshold = 2;

    // the regularizer lambda_l2 * ||V_i||_2^2
    optional float lambda_l2 = 3;

    // -- learning --

    // learning rate
    optional float lr_eta = 4 [default = .01];

    // V is initialized by uniformly random weight in
    //   [-init_scale, +init_scale]
    optional float init_scale = 6 [default = .01];

    // -- advanced --

    // leanring rate
    optional float lr_beta = 5 [default = 1];

    // if > 0, apply dropout on the gradient of V
    optional float dropout = 7 [default = 0];

    // if > 0, project the gradient of V into [-gc +gc]
    optional float grad_clipping = 8 [default = 0];

    // if > 0, normalized the gradient of V to 1 l2-norm
    optional float grad_normalization = 9 [default = 0];
  }

  repeated Embedding embedding = 13;

  // - learning -

  // the size of minibatch for minibatch SGD. a small value improves the
  // convergence rate, but may decrease the system performance
  optional int32 minibatch = 21 [default = 1000];

  // the maximal number of data passes
  optional int32 max_data_pass = 22 [default = 10];

  // stop the training early if the decrease of objective on the validation data
  // is less than *min_objv_decr*
  optional bool early_stop = 23 [default = false];

  optional float min_objv_decr = 24 [default = .00001];


  // -- advanced --

  // - data -

  // each worker reads the data in local if the data have been
  // dispatched into workers' local disks. it can reduce the cost to access data
  // remotely
  optional bool use_worker_local_data = 101;

  // virtually partition a file into nparts for better loadbalance.
  optional int32 num_parts_per_file = 102 [default = 10];

  // randomly shuffle data for minibatch SGD. a minibatch is randomly pick from
  // rand_shuffle * minibatch_size examples.
  optional int32 rand_shuffle = 103 [default = 10];

  // down sampling negative examples in the training data
  optional float neg_sampling = 104 [default = 1.0];

  // - learning -

  // print the progress every n sec
  optional float disp_itv = 111 [default = 1];

  // learning rate for w, see lr_eta
  optional float lr_beta = 112 [default = 1];

  // stop if the objective values exceeds *max_objv* (often used in parameter
  // tuning)
  optional float max_objv = 113;

  // force V_i = 0 if w_i = 0
  optional bool l1_shrk = 114 [default = true];

  // optional bool grad_normalization = 115 [default = false];

  // - system performance -

  // number of threads used within a worker and a server
  optional int32 num_threads = 121 [default = 2];

  // maximal allowed delay during synchronization. it is the maximal number of
  // parallel minibatches for SGD, and parallel block for block CD.
  optional int32 max_delay = 122 [default = 0];

  // cache the key list on both sender and receiver to reduce communication
  // cost. it may increase the memory usage
  optional bool key_cache = 123 [default = true];

  // compression the message to reduce communication cost. it may increase the
  // computation cost.
  optional bool msg_compression = 124 [default = true];

  // convert floating-points into fixed-point integers with n bytes. n can be 1,
  // 2 and 3. 0 means no compression.
  optional int32 fixed_bytes = 125 [default = 0];
}
